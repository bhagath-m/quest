{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS S3\n",
    "S3_BUCKET_NAME = \"rearc-quest-data-bhagath\"\n",
    "AWS_REGION = \"ap-south-1\"\n",
    "\n",
    "# DATASETS\n",
    "DATA_DIR = \"data\"\n",
    "DATASET1_URL = \"https://download.bls.gov/pub/time.series/pr\"\n",
    "DATASET1_HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:141.0) Gecko/20100101 Firefox/141.0\"}\n",
    "\n",
    "DATASET2_URL = \"https://honolulu-api.datausa.io/tesseract/data.jsonrecords?cube=acs_yg_total_population_1&drilldowns=Year%2CNation&locale=en&measures=Population\"\n",
    "DATASET2_JSON_FILE = \"usa_population.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name):\n",
    "    \"\"\"\n",
    "    Function to create an S3 bucket if it does not already exist.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3',region_name=AWS_REGION)\n",
    "    bucket_exists = any([ bucket[\"Name\"] for bucket in boto3.client('s3').list_buckets()[\"Buckets\"] \n",
    "                 if bucket[\"Name\"] == bucket_name])\n",
    "    if bucket_exists:\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    else:\n",
    "        print(f\"Creating bucket {bucket_name}\")\n",
    "        s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration={'LocationConstraint': AWS_REGION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset1_info():\n",
    "    \"\"\"\n",
    "    Function to retrive file, URL mappings for first dataset.\n",
    "    \"\"\"\n",
    "    response = requests.get(DATASET1_URL,headers=DATASET1_HEADERS)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    files_dict = {}\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if href and not href.endswith('/'):  # skip directories & parent links\n",
    "            filename = href.split('/')[-1]\n",
    "            files_dict[filename] = urljoin(DATASET1_URL, href)\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset2_json():\n",
    "    \"\"\"\n",
    "    Function to download the second dataset as JSON.\n",
    "    \"\"\"\n",
    "    response = requests.get(DATASET2_URL)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    download_dir = os.path.join(DATA_DIR, \"dataset2\")\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    with open(os.path.join(download_dir, DATASET2_JSON_FILE), 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Dataset 2 JSON saved to {DATASET2_JSON_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_objects(bucket_name,s3_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Function to get list of objects(files) in an S3 bucket.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3',region_name=AWS_REGION)\n",
    "    objects = []\n",
    "\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
    "        if 'Contents' in page:\n",
    "            for item in page['Contents']:\n",
    "                key = item['Key']\n",
    "                if not key.endswith('/'):  # skip folder marker keys\n",
    "                    if key.startswith(s3_prefix):\n",
    "                        key = key.split(s3_prefix)[1]\n",
    "                    objects.append(key)\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_s3(files_info, existing_s3_files, s3_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Function to upload files to S3 bucket from Dataset 1.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3',region_name=AWS_REGION)\n",
    "    files_to_download = [ \"pr.data.0.Current\" ]\n",
    "\n",
    "    download_dir = os.path.join(DATA_DIR, \"dataset1\")\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    for filename, file_url in files_info.items():\n",
    "        #skip filenames with no urls to download, usually these are not part of Dataset1\n",
    "        if file_url is None: continue\n",
    "        file_resp = None\n",
    "        if filename not in existing_s3_files:\n",
    "            file_resp = requests.get(file_url, stream=True, headers=DATASET1_HEADERS)\n",
    "            file_resp.raise_for_status()\n",
    "            print(f\"Uploading {filename} to S3...\")\n",
    "            s3.upload_fileobj(file_resp.raw, S3_BUCKET_NAME, f\"{s3_prefix}{filename}\")\n",
    "        else:\n",
    "            print(f\"Skipping {filename}, already in S3.\")\n",
    "        \n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        if filename in files_to_download and not os.path.exists(filepath):    \n",
    "            print(f\"Downloading {filename} to local directory...\")\n",
    "            if file_resp is None:\n",
    "                file_resp = requests.get(file_url, stream=True, headers=DATASET1_HEADERS)\n",
    "                file_resp.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                for chunk in file_resp.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_from_s3(files_info, existing_s3_files,s3_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Function to delete files from S3 bucket that are not present in Dataset 1.\n",
    "    \"\"\"\n",
    "    files_to_delete = set(existing_s3_files) - set(files_info.keys())\n",
    "    s3 = boto3.client('s3',region_name=AWS_REGION)\n",
    "    if files_to_delete:\n",
    "        print(\"Deleting obsolete files from S3...\")\n",
    "        delete_objects = [{\"Key\": f\"{s3_prefix}{f}\"} for f in files_to_delete]\n",
    "        s3.delete_objects(Bucket=S3_BUCKET_NAME, Delete={\"Objects\": delete_objects})\n",
    "        print(f\"Deleted {len(delete_objects)} files.\")\n",
    "    else:\n",
    "        print(\"No files to delete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index_html(prefix):\n",
    "    \"\"\"\n",
    "    Create an index.html to place in s3 and serve the bucket objects as static webpage\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)\n",
    "    if \"Contents\" not in response:\n",
    "        print(\"No files found in S3 folder.\")\n",
    "        return\n",
    "\n",
    "    html_lines = [\n",
    "        \"<!DOCTYPE html>\",\n",
    "        \"<html><head><title>Dataset Files</title></head><body>\",\n",
    "        f\"<h2>Files in {prefix}</h2>\",\n",
    "        \"<ul>\"\n",
    "    ]\n",
    "\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\"/\"):  # skip \"folder markers\"\n",
    "            continue\n",
    "        filename = key.split(\"/\")[-1]\n",
    "        file_url = f\"https://{S3_BUCKET_NAME}.s3.{AWS_REGION}.amazonaws.com/{key}\"\n",
    "        html_lines.append(f'<li><a href=\"{file_url}\">{filename}</a></li>')\n",
    "\n",
    "    html_lines.append(\"</ul></body></html>\")\n",
    "    html_content = \"\\n\".join(html_lines)\n",
    "\n",
    "    # upload index.html into the folder\n",
    "    index_key = prefix + \"index.html\"\n",
    "    s3.put_object(\n",
    "        Bucket=S3_BUCKET_NAME,\n",
    "        Key=index_key,\n",
    "        Body=html_content,\n",
    "        ContentType=\"text/html\"\n",
    "    )\n",
    "\n",
    "    print(f\"index.html uploaded to s3://{S3_BUCKET_NAME}/{index_key}\")\n",
    "    print(f\"Access it via: https://{S3_BUCKET_NAME}.s3.{AWS_REGION}.amazonaws.com/{index_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_bucket(s3_prefix=\"dataset/\"):\n",
    "    \"\"\"\n",
    "    Function to synchronize files with S3 bucket.\n",
    "    and uploads Dataset 2 JSON to S3.\n",
    "    \"\"\"\n",
    "    remote_files = get_dataset1_info()\n",
    "    s3_files = get_s3_objects(S3_BUCKET_NAME,s3_prefix)\n",
    "    remote_files[\"index.html\"] = None\n",
    "    remote_files[DATASET2_JSON_FILE] = None\n",
    "    upload_files_to_s3(remote_files,s3_files,s3_prefix)\n",
    "    remove_files_from_s3(remote_files,s3_files,s3_prefix)\n",
    "    \n",
    "    print(\"Uploading Dataset 2 JSON to S3...\")\n",
    "    get_dataset2_json()\n",
    "    dataset2_path = os.path.join(DATA_DIR, \"dataset2\", DATASET2_JSON_FILE)\n",
    "    s3 = boto3.client('s3',region_name=AWS_REGION)\n",
    "    s3.upload_file(dataset2_path, S3_BUCKET_NAME, f\"{s3_prefix}{DATASET2_JSON_FILE}\")\n",
    "    print(f\"Uploaded {DATASET2_JSON_FILE} to S3 bucket {S3_BUCKET_NAME}\")\n",
    "\n",
    "    print(\"S3 Sync complete.\")\n",
    "    generate_index_html(s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket rearc-quest-data-bhagath already exists\n"
     ]
    }
   ],
   "source": [
    "create_s3_bucket(S3_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping pr.class, already in S3.\n",
      "Skipping pr.contacts, already in S3.\n",
      "Skipping pr.data.0.Current, already in S3.\n",
      "Skipping pr.data.1.AllData, already in S3.\n",
      "Skipping pr.duration, already in S3.\n",
      "Skipping pr.footnote, already in S3.\n",
      "Skipping pr.measure, already in S3.\n",
      "Skipping pr.period, already in S3.\n",
      "Skipping pr.seasonal, already in S3.\n",
      "Skipping pr.sector, already in S3.\n",
      "Skipping pr.series, already in S3.\n",
      "Skipping pr.txt, already in S3.\n",
      "No files to delete.\n",
      "Uploading Dataset 2 JSON to S3...\n",
      "Dataset 2 JSON saved to usa_population.json\n",
      "Uploaded usa_population.json to S3 bucket rearc-quest-data-bhagath\n",
      "S3 Sync complete.\n",
      "index.html uploaded to s3://rearc-quest-data-bhagath/dataset/index.html\n",
      "Access it via: https://rearc-quest-data-bhagath.s3.ap-south-1.amazonaws.com/dataset/index.html\n"
     ]
    }
   ],
   "source": [
    "sync_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(DATA_DIR, \"dataset1\", \"pr.data.0.Current\"), sep=\"\\t\")\n",
    "df1.columns = df1.columns.str.strip()\n",
    "df1 = df1.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "ds2_json = json.load(open(os.path.join(DATA_DIR, \"dataset2\", DATASET2_JSON_FILE)))\n",
    "df2 = pd.json_normalize(ds2_json['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines = [\n",
    "        \"<!DOCTYPE html>\",\n",
    "        \"<html><head><title>Data Analytics</title></head><body>\",\n",
    "        f\"<h2>Data Analytics</h2>\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population Data statistics from 2013 to 2918\n",
      "Mean: 322069808.0\n",
      "Standard Deviation: 4158441.040908095\n"
     ]
    }
   ],
   "source": [
    "pop_df = df2[(df2['Year']>=2013) & (df2['Year']<=2018)]['Population']\n",
    "html_lines.append(\"<h3>Population Data statistics from 2013 to 2918</h3>\")\n",
    "html_lines.append(f\"<p>Mean: {pop_df.mean()}</p>\")\n",
    "html_lines.append(f\"<p>Standard Deviation: {pop_df.std()}</p>\")\n",
    "print(\"Population Data statistics from 2013 to 2918\")\n",
    "print(f\"Mean: {pop_df.mean()}\")\n",
    "print(f\"Standard Deviation: {pop_df.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRS30006011</td>\n",
       "      <td>1995</td>\n",
       "      <td>Q01</td>\n",
       "      <td>2.600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRS30006011</td>\n",
       "      <td>1995</td>\n",
       "      <td>Q02</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRS30006011</td>\n",
       "      <td>1995</td>\n",
       "      <td>Q03</td>\n",
       "      <td>0.900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRS30006011</td>\n",
       "      <td>1995</td>\n",
       "      <td>Q04</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRS30006011</td>\n",
       "      <td>1995</td>\n",
       "      <td>Q05</td>\n",
       "      <td>1.400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37177</th>\n",
       "      <td>PRS88003203</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q02</td>\n",
       "      <td>116.544</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37178</th>\n",
       "      <td>PRS88003203</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q03</td>\n",
       "      <td>116.593</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37179</th>\n",
       "      <td>PRS88003203</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q04</td>\n",
       "      <td>116.682</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37180</th>\n",
       "      <td>PRS88003203</td>\n",
       "      <td>2024</td>\n",
       "      <td>Q05</td>\n",
       "      <td>116.686</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37181</th>\n",
       "      <td>PRS88003203</td>\n",
       "      <td>2025</td>\n",
       "      <td>Q01</td>\n",
       "      <td>118.271</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37182 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         series_id  year period    value footnote_codes\n",
       "0      PRS30006011  1995    Q01    2.600            NaN\n",
       "1      PRS30006011  1995    Q02    2.100            NaN\n",
       "2      PRS30006011  1995    Q03    0.900            NaN\n",
       "3      PRS30006011  1995    Q04    0.100            NaN\n",
       "4      PRS30006011  1995    Q05    1.400            NaN\n",
       "...            ...   ...    ...      ...            ...\n",
       "37177  PRS88003203  2024    Q02  116.544            NaN\n",
       "37178  PRS88003203  2024    Q03  116.593            NaN\n",
       "37179  PRS88003203  2024    Q04  116.682            NaN\n",
       "37180  PRS88003203  2024    Q05  116.686            NaN\n",
       "37181  PRS88003203  2025    Q01  118.271              R\n",
       "\n",
       "[37182 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_by_sid_yr = df1.groupby(['series_id','year'], as_index=False)['value'].sum()\n",
    "df1_by_best_yr = df1_by_sid_yr.loc[df1_by_sid_yr.groupby(\"series_id\")[\"value\"].idxmax()].reset_index(drop=True)\n",
    "html_lines.append(\"<h3>Best year info per series_id</h3>\")\n",
    "html_lines.append(df1_by_best_yr.to_html(index=False))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRS30006032</td>\n",
       "      <td>2018</td>\n",
       "      <td>Q01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>327167439.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year period  value   Population\n",
       "0  PRS30006032  2018    Q01    0.5  327167439.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question: the requirement is not clear about the input year, \n",
    "# I took value 2018 as it's shown in the example/expected output\n",
    "# I made these 3 params configurable as I am not sure if the series_id, period given in REAM me is just for reference\n",
    "# or if it should be used in the solution to filter out Dataset 1 i.e. df1.\n",
    "series_id = 'PRS30006032'\n",
    "period = 'Q01'\n",
    "year = 2018\n",
    "\n",
    "\n",
    "filtered_df1 = df1[(df1['series_id'] == series_id) \n",
    "                   & (df1['period'] == period)\n",
    "                   & (df1['year'] == year)]\n",
    "merged_df = pd.merge(filtered_df1,df2,\n",
    "         left_on=['year'],\n",
    "         right_on=['Year'],\n",
    "         how='inner')[['series_id','year','period','value','Population']]\n",
    "html_lines.append(f\"<h3>Population details for series_id: {series_id}, period: {period}, year: {year} </h3>\")\n",
    "html_lines.append(merged_df.to_html(index=False))\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append(\"</body></html>\")\n",
    "html_content = \"\\n\".join(html_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved at data/reports/report.html\n"
     ]
    }
   ],
   "source": [
    "report_dir = os.path.join(DATA_DIR,\"reports\")\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "html_path = os.path.join(report_dir,\"report.html\")\n",
    "with open(html_path,\"w\") as html_file:\n",
    "    html_file.write(html_content)\n",
    "print(f\"Report saved at {html_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
